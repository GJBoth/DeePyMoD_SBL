{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "from deepmod_l1.analytical import theta_analytical\n",
    "\n",
    "#Plotting imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sample = np.linspace(-5, 5, 50, dtype=np.float32)\n",
    "t_sample = np.linspace(0, 5, 50, dtype=np.float32)\n",
    "x_grid, t_grid = np.meshgrid(x_sample, t_sample, indexing='ij')\n",
    "    \n",
    "# Analytical\n",
    "time_deriv, theta_noiseless = theta_analytical(x_grid, t_grid, 0.5, 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V1; no convergence and selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = np.var(time_deriv) * 1e-3\n",
    "normalization = np.linalg.norm(theta_noiseless, axis=0)\n",
    "theta = theta_noiseless / normalization\n",
    "t = time_deriv + np.random.normal(scale= np.sqrt(noise), size=time_deriv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(theta, t):\n",
    "    # Noise level\n",
    "    beta = 1 / (np.var(t) * 0.1) # beta = 1/sigma^2\n",
    "    \n",
    "    # Finding best initial vector \n",
    "    projection = np.concatenate([((phi_i[:, None].T @ t).T @ (phi_i[:, None].T @ t)) / (phi_i[:, None].T @ phi_i[:, None]) for phi_i in theta.T])\n",
    "    start_idx = np.argmax(projection)\n",
    "\n",
    "    # Initializing alphas\n",
    "    alfa = np.ones((theta.shape[1], 1)) * np.inf\n",
    "    alfa[start_idx] = theta[:, start_idx:start_idx+1].T @ theta[:, start_idx:start_idx+1] / (projection[start_idx] - 1/beta)\n",
    "    Phi = theta[:, [start_idx]]\n",
    "    \n",
    "    return Phi, alfa, beta\n",
    "\n",
    "def posterior(Phi, t, alfa, beta):\n",
    "    Sigma = np.linalg.inv(alfa[alfa != np.inf] * np.eye(Phi.shape[1]) + beta * Phi.T @ Phi)  # posterior covariance\n",
    "    mu = beta * Sigma @ Phi.T @ t  # posterior mean\n",
    "    \n",
    "    return Sigma, mu\n",
    "\n",
    "def sparse_quality_factor(theta, Phi, Sigma, alfa, beta):\n",
    "    B = beta * np.eye(Phi.shape[0])\n",
    "    precalc = B @ Phi @ Sigma @ Phi.T @ B\n",
    "\n",
    "    Sm = np.concatenate([phi_i[:, None].T @ B @ phi_i[:, None] - phi_i[:, None].T @ precalc @ phi_i[:, None] for phi_i in theta.T])\n",
    "    Qm = np.concatenate([phi_i[:, None].T @ B @ t - phi_i[:, None].T @ precalc @ t for phi_i in theta.T])\n",
    "\n",
    "    sm = Sm/(1 - Sm/alfa)\n",
    "    qm = Qm/(1 - Sm/alfa)\n",
    "    \n",
    "    return sm, qm\n",
    "\n",
    "def update_design_matrix(theta, sm, qm, alfa):\n",
    "    idx = np.random.choice(theta.shape[1])\n",
    "    phi_i = theta[:, idx:idx+1]\n",
    "    theta_i = qm[idx, 0]**2 - sm[idx, 0]\n",
    "    \n",
    "    # Decididing what to do \n",
    "    if (theta_i > 0) & (alfa[idx, 0] != np.inf):\n",
    "        alfa[idx, 0] = sm[idx, 0]**2 / theta_i #reestimating\n",
    "    elif (theta_i > 0) & (alfa[idx, 0] == np.inf):\n",
    "        alfa[idx, 0] = sm[idx, 0]**2 / theta_i # adding alpha\n",
    "    elif (theta_i< 0) & (alfa[idx, 0] != np.inf):\n",
    "        alfa[idx, 0] = np.inf #removing alpha\n",
    "    \n",
    "    Phi = theta[:, alfa[:, 0] != np.inf] #rebuilding phi\n",
    "        \n",
    "    return Phi, alfa\n",
    "\n",
    "def update_noise(Phi, t, mu, Sigma, alfa): \n",
    "    beta = (Phi.shape[0] - Phi.shape[1] + np.sum(alfa[alfa != np.inf] * np.diag(Sigma))) / ((t - Phi @ mu).T @ (t - Phi @ mu)) \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing\n",
    "Phi, alfa, beta = initialize(theta, t)\n",
    "Sigma, mu = posterior(Phi, t, alfa, beta)\n",
    "sm, qm = sparse_quality_factor(theta, Phi, Sigma, alfa, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[           inf]\n",
      " [           inf]\n",
      " [3.26772296e-03]\n",
      " [2.64097274e+03]\n",
      " [           inf]\n",
      " [           inf]\n",
      " [           inf]\n",
      " [           inf]\n",
      " [1.00321113e+03]]\n",
      "[[           inf]\n",
      " [           inf]\n",
      " [3.26772297e-03]\n",
      " [2.64097274e+03]\n",
      " [           inf]\n",
      " [           inf]\n",
      " [           inf]\n",
      " [           inf]\n",
      " [1.00321113e+03]]\n",
      "[[           inf]\n",
      " [           inf]\n",
      " [3.26772296e-03]\n",
      " [2.64097274e+03]\n",
      " [           inf]\n",
      " [           inf]\n",
      " [           inf]\n",
      " [           inf]\n",
      " [1.00321113e+03]]\n"
     ]
    }
   ],
   "source": [
    "for it in np.arange(150):\n",
    "    Phi, alfa = update_design_matrix(theta, sm, qm, alfa)\n",
    "    Sigma, mu = posterior(Phi, t, alfa, beta)\n",
    "    sm, qm = sparse_quality_factor(theta, Phi, Sigma, alfa, beta)\n",
    "    beta = update_noise(Phi, t, mu, Sigma, alfa)\n",
    "    if it % 50 == 0:\n",
    "        print(alfa)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0001187]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00012206738442182541"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = alfa[alfa != np.inf] * np.eye(Phi.shape[1]) + beta * Phi.T @ Phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.914873305538197"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.cond(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[           inf],\n",
       "       [           inf],\n",
       "       [3.26772296e-03],\n",
       "       [2.64097274e+03],\n",
       "       [           inf],\n",
       "       [           inf],\n",
       "       [           inf],\n",
       "       [           inf],\n",
       "       [1.00321113e+03]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.0069935e-01, -2.2107922e-03, -4.3951401e-04]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu.T / normalization[alfa[:, 0] != np.inf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.27172706e+03],\n",
       "       [-8.01136850e+03],\n",
       "       [ 2.83764020e+09],\n",
       "       [ 2.21528893e+04],\n",
       "       [-2.31155327e+03],\n",
       "       [-5.77339929e+01],\n",
       "       [-2.27447916e+01],\n",
       "       [-3.37625071e+03],\n",
       "       [ 5.80808428e+03]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qm**2 - sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = np.var(time_deriv) * 1e-3\n",
    "normalization = np.linalg.norm(theta_noiseless, axis=0)\n",
    "theta = theta_noiseless / normalization\n",
    "t = time_deriv + np.random.normal(scale= np.sqrt(noise), size=time_deriv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(theta, t):\n",
    "    # Noise level\n",
    "    beta = 1 / (np.var(t) * 0.1) # beta = 1/sigma^2\n",
    "    \n",
    "    # Finding best initial vector \n",
    "    projection = np.concatenate([((phi_i[:, None].T @ t).T @ (phi_i[:, None].T @ t)) / (phi_i[:, None].T @ phi_i[:, None]) for phi_i in theta.T])\n",
    "    start_idx = np.argmax(projection)\n",
    "\n",
    "    # Initializing alphas\n",
    "    alfa = np.ones((theta.shape[1], 1)) * np.inf\n",
    "    alfa[start_idx] = theta[:, start_idx:start_idx+1].T @ theta[:, start_idx:start_idx+1] / (projection[start_idx] - 1/beta)\n",
    "    Phi = theta[:, [start_idx]]\n",
    "    \n",
    "    return Phi, alfa, beta\n",
    "\n",
    "def posterior(Phi, t, alfa, beta):\n",
    "    Sigma = np.linalg.inv(alfa[alfa != np.inf] * np.eye(Phi.shape[1]) + beta * Phi.T @ Phi)  # posterior covariance\n",
    "    mu = beta * Sigma @ Phi.T @ t  # posterior mean\n",
    "    \n",
    "    return Sigma, mu\n",
    "\n",
    "def sparse_quality_factor(theta, Phi, Sigma, alfa, beta):\n",
    "    B = beta * np.eye(Phi.shape[0])\n",
    "    precalc = B @ Phi @ Sigma @ Phi.T @ B\n",
    "\n",
    "    Sm = np.concatenate([phi_i[:, None].T @ B @ phi_i[:, None] - phi_i[:, None].T @ precalc @ phi_i[:, None] for phi_i in theta.T])\n",
    "    Qm = np.concatenate([phi_i[:, None].T @ B @ t - phi_i[:, None].T @ precalc @ t for phi_i in theta.T])\n",
    "\n",
    "    sm = Sm/(1 - Sm/alfa)\n",
    "    qm = Qm/(1 - Sm/alfa)\n",
    "    \n",
    "    return sm, qm\n",
    "\n",
    "def update_design_matrix(theta, sm, qm, alfa):\n",
    "    idx = np.random.choice(theta.shape[1])\n",
    "    phi_i = theta[:, idx:idx+1]\n",
    "    theta_i = qm[idx, 0]**2 - sm[idx, 0]\n",
    "    \n",
    "    # Decididing what to do \n",
    "    if (theta_i > 0) & (alfa[idx, 0] != np.inf):\n",
    "        alfa[idx, 0] = sm[idx, 0]**2 / theta_i #reestimating\n",
    "    elif (theta_i > 0) & (alfa[idx, 0] == np.inf):\n",
    "        alfa[idx, 0] = sm[idx, 0]**2 / theta_i # adding alpha\n",
    "    elif (theta_i< 0) & (alfa[idx, 0] != np.inf):\n",
    "        alfa[idx, 0] = np.inf #removing alpha\n",
    "    \n",
    "    Phi = theta[:, alfa[:, 0] != np.inf] #rebuilding phi\n",
    "        \n",
    "    return Phi, alfa\n",
    "\n",
    "def update_noise(Phi, t, mu, Sigma, alfa): \n",
    "    beta = (Phi.shape[0] - Phi.shape[1] + np.sum(alfa[alfa != np.inf] * np.diag(Sigma))) / ((t - Phi @ mu).T @ (t - Phi @ mu)) \n",
    "    return beta\n",
    "\n",
    "def convergence(sm, qm, alfa):\n",
    "    dt = qm**2 - sm\n",
    "    delta_alfa= sm**2 / dt - alfa # check a_new - a\n",
    "    converged = np.max(np.abs(delta_alfa[dt > 0])) < 10**-6 # if max delta_a < 10^-6 and all other dt < 0, it has converged\n",
    "    \n",
    "    return converged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing\n",
    "Phi, alfa, beta = initialize(theta, t)\n",
    "Sigma, mu = posterior(Phi, t, alfa, beta)\n",
    "sm, qm = sparse_quality_factor(theta, Phi, Sigma, alfa, beta)\n",
    "\n",
    "converged = False\n",
    "while converged == False:\n",
    "    Phi, alfa = update_design_matrix(theta, sm, qm, alfa)\n",
    "    Sigma, mu = posterior(Phi, t, alfa, beta)\n",
    "    sm, qm = sparse_quality_factor(theta, Phi, Sigma, alfa, beta)\n",
    "    beta = update_noise(Phi, t, mu, Sigma, alfa)\n",
    "    converged = convergence(sm, qm, alfa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[           inf],\n",
       "       [           inf],\n",
       "       [3.26702711e-03],\n",
       "       [7.02437222e+03],\n",
       "       [3.97346431e+03],\n",
       "       [           inf],\n",
       "       [           inf],\n",
       "       [           inf],\n",
       "       [           inf]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.74953866e+01],\n",
       "       [8.61199757e-03],\n",
       "       [1.30597120e-02]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.93805820e+03],\n",
       "       [-2.80261667e+03],\n",
       "       [ 1.92123330e+10],\n",
       "       [ 8.30841866e+03],\n",
       "       [ 1.75678116e+04],\n",
       "       [-1.41402957e+03],\n",
       "       [-2.88497653e+03],\n",
       "       [-1.66577832e+03],\n",
       "       [-1.86177632e+03]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qm**2 - sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Including choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = np.var(time_deriv) * 0.1\n",
    "normalization = np.linalg.norm(theta_noiseless, axis=0)\n",
    "theta = theta_noiseless / normalization\n",
    "t = time_deriv + np.random.normal(scale= np.sqrt(noise), size=time_deriv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(theta, t):\n",
    "    # Noise level\n",
    "    beta = 1 / (np.var(t) * 0.1) # beta = 1/sigma^2\n",
    "    \n",
    "    # Finding best initial vector \n",
    "    projection = np.concatenate([((phi_i[:, None].T @ t).T @ (phi_i[:, None].T @ t)) / (phi_i[:, None].T @ phi_i[:, None]) for phi_i in theta.T])\n",
    "    start_idx = np.argmax(projection)\n",
    "\n",
    "    # Initializing alphas\n",
    "    alfa = np.ones((theta.shape[1], 1)) * np.inf\n",
    "    alfa[start_idx] = theta[:, start_idx:start_idx+1].T @ theta[:, start_idx:start_idx+1] / (projection[start_idx] - 1/beta)\n",
    "    Phi = theta[:, [start_idx]]\n",
    "    \n",
    "    return Phi, alfa, beta\n",
    "\n",
    "def posterior(Phi, t, alfa, beta):\n",
    "    Sigma = np.linalg.inv(alfa[alfa != np.inf] * np.eye(Phi.shape[1]) + beta * Phi.T @ Phi)  # posterior covariance\n",
    "    mu = beta * Sigma @ Phi.T @ t  # posterior mean\n",
    "    \n",
    "    return Sigma, mu\n",
    "\n",
    "def sparse_quality_factor(theta, Phi, Sigma, alfa, beta):\n",
    "    B = beta * np.eye(Phi.shape[0])\n",
    "    precalc = B @ Phi @ Sigma @ Phi.T @ B\n",
    "\n",
    "    Sm = np.concatenate([phi_i[:, None].T @ B @ phi_i[:, None] - phi_i[:, None].T @ precalc @ phi_i[:, None] for phi_i in theta.T])\n",
    "    Qm = np.concatenate([phi_i[:, None].T @ B @ t - phi_i[:, None].T @ precalc @ t for phi_i in theta.T])\n",
    "\n",
    "    sm = Sm/(1 - Sm/alfa)\n",
    "    qm = Qm/(1 - Sm/alfa)\n",
    "    \n",
    "    return sm, qm, Sm, Qm \n",
    "\n",
    "def update_design_matrix(theta, sm, qm, alfa, Sm, Qm):\n",
    "    idx = optimal_vec(sm, qm, Sm, Qm, alfa)\n",
    "    phi_i = theta[:, idx:idx+1]\n",
    "    theta_i = qm[idx, 0]**2 - sm[idx, 0]\n",
    "    \n",
    "    # Decididing what to do \n",
    "    if (theta_i > 0) & (alfa[idx, 0] != np.inf):\n",
    "        alfa[idx, 0] = sm[idx, 0]**2 / theta_i #reestimating\n",
    "    elif (theta_i > 0) & (alfa[idx, 0] == np.inf):\n",
    "        alfa[idx, 0] = sm[idx, 0]**2 / theta_i # adding alpha\n",
    "    elif (theta_i< 0) & (alfa[idx, 0] != np.inf):\n",
    "        alfa[idx, 0] = np.inf #removing alpha\n",
    "    \n",
    "    Phi = theta[:, alfa[:, 0] != np.inf] #rebuilding phi\n",
    "        \n",
    "    return Phi, alfa\n",
    "\n",
    "def update_noise(Phi, t, mu, Sigma, alfa): \n",
    "    beta = (Phi.shape[0] - Phi.shape[1] + np.sum(alfa[alfa != np.inf] * np.diag(Sigma))) / ((t - Phi @ mu).T @ (t - Phi @ mu)) \n",
    "    return beta\n",
    "\n",
    "def convergence(sm, qm, alfa):\n",
    "    dt = qm**2 - sm\n",
    "    delta_alfa= sm**2 / dt - alfa # check a_new - a\n",
    "    converged = np.max(np.abs(delta_alfa[dt > 0])) < 10**-6 # if max delta_a < 10^-6 and all other dt < 0, it has converged\n",
    "    \n",
    "    return converged\n",
    "\n",
    "def optimal_vec(sm, qm, Sm, Qm, alfa):\n",
    "    basis_idx = alfa != np.inf  # idx of bases in model\n",
    "    set_idx = alfa == np.inf  # idx of bases not in model\n",
    "    \n",
    "    add_basis = (Qm**2 - Sm)/Sm + np.log(Sm/Qm**2)\n",
    "    del_basis = Qm**2/(Sm - alfa) - np.log(1-Sm/alfa)\n",
    "    alfa_new = sm**2/(qm**2 - sm)\n",
    "    redo_basis = Qm**2/(Sm + (1/alfa_new-1/alfa)**-1) - np.log(1 + Sm * (1/alfa_new-1/alfa))\n",
    "    \n",
    "    #Making everything into nice matrix\n",
    "    add_basis[basis_idx] = np.nan\n",
    "    dt = qm**2 - sm\n",
    "    add_basis[dt <= 0] = np.nan #stuff above assumes dt > 0\n",
    "    del_basis[set_idx] = np.nan\n",
    "    redo_basis[set_idx] = np.nan\n",
    "    \n",
    "    # Deciding update\n",
    "    possible_update = np.concatenate((add_basis, redo_basis, del_basis), axis=1)\n",
    "  \n",
    "    idx = np.unravel_index(np.nanargmax(possible_update), possible_update.shape)[0]\n",
    "    \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing\n",
    "Phi, alfa, beta = initialize(theta, t)\n",
    "Sigma, mu = posterior(Phi, t, alfa, beta)\n",
    "sm, qm, Sm, Qm = sparse_quality_factor(theta, Phi, Sigma, alfa, beta)\n",
    "\n",
    "converged = False\n",
    "while converged == False:\n",
    "    Phi, alfa = update_design_matrix(theta, sm, qm, alfa, Sm, Qm)\n",
    "    Sigma, mu = posterior(Phi, t, alfa, beta)\n",
    "    sm, qm, Sm, Qm = sparse_quality_factor(theta, Phi, Sigma, alfa, beta)\n",
    "    beta = update_noise(Phi, t, mu, Sigma, alfa)\n",
    "    converged = convergence(sm, qm, alfa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.50114911],\n",
       "       [0.00766854]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu / normalization[alfa[:, 0] != np.inf][:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1220.6755 ,  265.13342],\n",
       "       [ 265.13342,   57.58757]], dtype=float32)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0128321 , 0.00154697],\n",
       "       [0.00154697, 0.00528659]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[       inf],\n",
       "       [       inf],\n",
       "       [0.00327703],\n",
       "       [       inf],\n",
       "       [       inf],\n",
       "       [       inf],\n",
       "       [       inf],\n",
       "       [       inf],\n",
       "       [       inf]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alfa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00120296]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0012206738442182541"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:67: RuntimeWarning: invalid value encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1)\n",
      "(2, 1)\n",
      "(4, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(3, 1)\n"
     ]
    }
   ],
   "source": [
    "for idx in np.arange(10):\n",
    "    noise = np.var(time_deriv) * 0.1\n",
    "    normalization = np.linalg.norm(theta_noiseless, axis=0)\n",
    "    theta = theta_noiseless / normalization\n",
    "    t = time_deriv + np.random.normal(scale= np.sqrt(noise), size=time_deriv.shape)\n",
    "    \n",
    "    # Initializing\n",
    "    Phi, alfa, beta = initialize(theta, t)\n",
    "    Sigma, mu = posterior(Phi, t, alfa, beta)\n",
    "    sm, qm, Sm, Qm = sparse_quality_factor(theta, Phi, Sigma, alfa, beta)\n",
    "\n",
    "    converged = False\n",
    "    while converged == False:\n",
    "        Phi, alfa = update_design_matrix(theta, sm, qm, alfa, Sm, Qm)\n",
    "        Sigma, mu = posterior(Phi, t, alfa, beta)\n",
    "        sm, qm, Sm, Qm = sparse_quality_factor(theta, Phi, Sigma, alfa, beta)\n",
    "        beta = update_noise(Phi, t, mu, Sigma, alfa)\n",
    "        converged = convergence(sm, qm, alfa)\n",
    "    print(mu.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:67: RuntimeWarning: invalid value encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17.54688182]\n",
      " [ 0.0585946 ]\n",
      " [-0.16712367]]\n",
      "[[17.54688182]\n",
      " [ 0.0585946 ]\n",
      " [-0.16712367]]\n",
      "[[17.54688182]\n",
      " [ 0.0585946 ]\n",
      " [-0.16712367]]\n",
      "[[17.54688182]\n",
      " [ 0.0585946 ]\n",
      " [-0.16712367]]\n",
      "[[17.54688182]\n",
      " [ 0.0585946 ]\n",
      " [-0.16712367]]\n",
      "[[17.54688182]\n",
      " [ 0.0585946 ]\n",
      " [-0.16712367]]\n",
      "[[17.54688182]\n",
      " [ 0.0585946 ]\n",
      " [-0.16712367]]\n",
      "[[17.54688182]\n",
      " [ 0.0585946 ]\n",
      " [-0.16712367]]\n",
      "[[17.54688182]\n",
      " [ 0.0585946 ]\n",
      " [-0.16712367]]\n",
      "[[17.54688182]\n",
      " [ 0.0585946 ]\n",
      " [-0.16712367]]\n"
     ]
    }
   ],
   "source": [
    "noise = np.var(time_deriv) * 0.05\n",
    "normalization = np.linalg.norm(theta_noiseless, axis=0)\n",
    "theta = theta_noiseless / normalization\n",
    "t = time_deriv + np.random.normal(scale= np.sqrt(noise), size=time_deriv.shape)\n",
    "\n",
    "for idx in np.arange(10):\n",
    "    # Initializing\n",
    "    Phi, alfa, beta = initialize(theta, t)\n",
    "    Sigma, mu = posterior(Phi, t, alfa, beta)\n",
    "    sm, qm, Sm, Qm = sparse_quality_factor(theta, Phi, Sigma, alfa, beta)\n",
    "\n",
    "    converged = False\n",
    "    while converged == False:\n",
    "        Phi, alfa = update_design_matrix(theta, sm, qm, alfa, Sm, Qm)\n",
    "        Sigma, mu = posterior(Phi, t, alfa, beta)\n",
    "        sm, qm, Sm, Qm = sparse_quality_factor(theta, Phi, Sigma, alfa, beta)\n",
    "        beta = update_noise(Phi, t, mu, Sigma, alfa)\n",
    "        converged = convergence(sm, qm, alfa)\n",
    "    print(mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01297155]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9408.692"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalization @ normalization.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(theta, t):\n",
    "    # Noise level\n",
    "    beta = 1 / (np.var(t) * 0.1) # beta = 1/sigma^2\n",
    "    \n",
    "    # Finding best initial vector \n",
    "    projection = np.concatenate([((phi_i[:, None].T @ t).T @ (phi_i[:, None].T @ t)) / (phi_i[:, None].T @ phi_i[:, None]) for phi_i in theta.T])\n",
    "    start_idx = np.argmax(projection)\n",
    "\n",
    "    # Initializing alphas\n",
    "    alfa = np.ones((theta.shape[1], 1)) * np.inf\n",
    "    alfa[start_idx] = theta[:, start_idx:start_idx+1].T @ theta[:, start_idx:start_idx+1] / (projection[start_idx] - 1/beta)\n",
    "    Phi = theta[:, [start_idx]]\n",
    "    \n",
    "    return Phi, alfa, beta\n",
    "\n",
    "def posterior(Phi, t, alfa, beta):\n",
    "    Sigma = np.linalg.inv(alfa[alfa != np.inf] * np.eye(Phi.shape[1]) + beta * Phi.T @ Phi)  # posterior covariance\n",
    "    mu = beta * Sigma @ Phi.T @ t  # posterior mean\n",
    "    \n",
    "    return Sigma, mu\n",
    "\n",
    "def sparse_quality_factor(theta, Phi, Sigma, alfa, beta):\n",
    "    B = beta * np.eye(Phi.shape[0])\n",
    "    precalc = B @ Phi @ Sigma @ Phi.T @ B\n",
    "\n",
    "    Sm = np.concatenate([phi_i[:, None].T @ B @ phi_i[:, None] - phi_i[:, None].T @ precalc @ phi_i[:, None] for phi_i in theta.T])\n",
    "    Qm = np.concatenate([phi_i[:, None].T @ B @ t - phi_i[:, None].T @ precalc @ t for phi_i in theta.T])\n",
    "\n",
    "    sm = Sm/(1 - Sm/alfa)\n",
    "    qm = Qm/(1 - Sm/alfa)\n",
    "    \n",
    "    return sm, qm, Sm, Qm \n",
    "\n",
    "def update_design_matrix(theta, sm, qm, alfa, Sm, Qm):\n",
    "    idx = optimal_vec(sm, qm, Sm, Qm, alfa)\n",
    "    phi_i = theta[:, idx:idx+1]\n",
    "    theta_i = qm[idx, 0]**2 - sm[idx, 0]\n",
    "    \n",
    "    # Decididing what to do \n",
    "    if (theta_i > 0) & (alfa[idx, 0] != np.inf):\n",
    "        alfa[idx, 0] = sm[idx, 0]**2 / theta_i #reestimating\n",
    "    elif (theta_i > 0) & (alfa[idx, 0] == np.inf):\n",
    "        alfa[idx, 0] = sm[idx, 0]**2 / theta_i # adding alpha\n",
    "    elif (theta_i< 0) & (alfa[idx, 0] != np.inf):\n",
    "        alfa[idx, 0] = np.inf #removing alpha\n",
    "    \n",
    "    Phi = theta[:, alfa[:, 0] != np.inf] #rebuilding phi\n",
    "        \n",
    "    return Phi, alfa\n",
    "\n",
    "def update_noise(Phi, t, mu, Sigma, alfa): \n",
    "    beta = (Phi.shape[0] - Phi.shape[1] + np.sum(alfa[alfa != np.inf] * np.diag(Sigma))) / ((t - Phi @ mu).T @ (t - Phi @ mu)) \n",
    "    return beta\n",
    "\n",
    "def convergence(sm, qm, alfa):\n",
    "    dt = qm**2 - sm\n",
    "    delta_alfa= sm**2 / dt - alfa # check a_new - a\n",
    "    converged = np.max(np.abs(delta_alfa[dt > 0])) < 10**-6 # if max delta_a < 10^-6 and all other dt < 0, it has converged\n",
    "    \n",
    "    return converged\n",
    "\n",
    "def optimal_vec(sm, qm, Sm, Qm, alfa):\n",
    "    basis_idx = alfa != np.inf  # idx of bases in model\n",
    "    set_idx = alfa == np.inf  # idx of bases not in model\n",
    "    \n",
    "    add_basis = (Qm**2 - Sm)/Sm + np.log(Sm/Qm**2)\n",
    "    del_basis = Qm**2/(Sm - alfa) - np.log(1-Sm/alfa)\n",
    "    alfa_new = sm**2/(qm**2 - sm)\n",
    "    redo_basis = Qm**2/(Sm + (1/alfa_new-1/alfa)**-1) - np.log(1 + Sm * (1/alfa_new-1/alfa))\n",
    "    \n",
    "    #Making everything into nice matrix\n",
    "    add_basis[basis_idx] = np.nan\n",
    "    dt = qm**2 - sm\n",
    "    add_basis[dt <= 0] = np.nan #stuff above assumes dt > 0\n",
    "    del_basis[set_idx] = np.nan\n",
    "    redo_basis[set_idx] = np.nan\n",
    "    \n",
    "    # Deciding update\n",
    "    possible_update = np.concatenate((add_basis, redo_basis, del_basis), axis=1)\n",
    "  \n",
    "    idx = np.unravel_index(np.nanargmax(possible_update), possible_update.shape)[0]\n",
    "    \n",
    "    return idx\n",
    "\n",
    "def SBL(theta, t):\n",
    "    # Normalizing\n",
    "    normalization = np.linalg.norm(theta, axis=0)\n",
    "    theta_normalized = theta / normalization\n",
    "    \n",
    "    # Initializing\n",
    "    Phi, alfa, beta = initialize(theta_normalized, t)\n",
    "    Sigma, mu = posterior(Phi, t, alfa, beta)\n",
    "    sm, qm, Sm, Qm = sparse_quality_factor(theta_normalized, Phi, Sigma, alfa, beta)\n",
    "    \n",
    "    # Running\n",
    "    converged = False\n",
    "    while converged == False:\n",
    "        Phi, alfa = update_design_matrix(theta_normalized, sm, qm, alfa, Sm, Qm)\n",
    "        Sigma, mu = posterior(Phi, t, alfa, beta)\n",
    "        sm, qm, Sm, Qm = sparse_quality_factor(theta_normalized, Phi, Sigma, alfa, beta)\n",
    "        beta = update_noise(Phi, t, mu, Sigma, alfa)\n",
    "        converged = convergence(sm, qm, alfa)\n",
    "    \n",
    "    # Rescaling\n",
    "    factor = normalization[alfa[:, 0] != np.inf][:, None] \n",
    "    mu = mu / factor\n",
    "    Sigma = Sigma / (factor @ factor.T)\n",
    "    \n",
    "    \n",
    "    return alfa, mu, Sigma, 1/beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = np.var(time_deriv) * 0.1\n",
    "normalization = np.linalg.norm(theta_noiseless, axis=0)\n",
    "t = time_deriv + np.random.normal(scale= np.sqrt(noise), size=time_deriv.shape)\n",
    "\n",
    "alfa, mu, Sigma, inferred_noise = SBL(theta_noiseless, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[           inf],\n",
       "       [1.88373536e+01],\n",
       "       [3.32177549e-03],\n",
       "       [           inf],\n",
       "       [           inf],\n",
       "       [           inf],\n",
       "       [           inf],\n",
       "       [1.02419687e+01],\n",
       "       [           inf]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02229136],\n",
       "       [ 0.49659897],\n",
       "       [ 0.03555032]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.87453256e-04,  8.90290495e-21, -1.19554210e-04],\n",
       "       [ 7.96345568e-21,  1.01373825e-05, -1.13744648e-20],\n",
       "       [-1.19554210e-04, -1.32621504e-20,  2.46095182e-04]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
