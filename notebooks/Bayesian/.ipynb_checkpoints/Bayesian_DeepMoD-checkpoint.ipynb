{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we implement a Bayesian version of DeepMoD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from deepmod_l1.analytical import theta_analytical\n",
    "\n",
    "#Plotting imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# DeepMoD stuff\n",
    "from deepymod_torch.DeepMod import DeepMod, build_network\n",
    "from deepymod_torch.library_functions import library_basic\n",
    "from deepymod_torch.utilities import create_deriv_data\n",
    "from deepymod_torch.output import progress\n",
    "\n",
    "# Remainder imports\n",
    "from os import listdir, path, getcwd\n",
    "\n",
    "# Setting cuda\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "# Settings for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Defining output folder\n",
    "output_folder = getcwd()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('../../tests/data/burgers.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.transpose((data['t'].flatten(), data['x'].flatten()))\n",
    "y = np.real(data['u']).reshape((data['u'].size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_samples = 500\n",
    "\n",
    "idx = np.random.permutation(y.size)\n",
    "X_train = torch.tensor(X[idx, :][:number_of_samples], dtype=torch.float32, requires_grad=True)\n",
    "y_train = torch.tensor(y[idx, :][:number_of_samples], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adapting deepmod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just need to slightly modify the fitting layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Library(nn.Module):\n",
    "    '''Abstract baseclass for library-as-layer. Child requires theta function (see library_functions). '''\n",
    "    def __init__(self, input_dim, output_dim, diff_order):\n",
    "        super().__init__()\n",
    "        self.diff_order = diff_order\n",
    "        self.total_terms = self.terms(input_dim, output_dim, self.diff_order)\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''Calculates output.'''\n",
    "        time_deriv_list, theta = self.theta(input)\n",
    "        return input, time_deriv_list, theta\n",
    "\n",
    "    def terms(self, input_dim, output_dim, max_order):\n",
    "        '''Calculates the number of terms the library produces'''\n",
    "        sample_data = (torch.ones((1, output_dim), dtype=torch.float32), torch.ones((1, max_order, input_dim, output_dim), dtype=torch.float32)) # we run a single forward pass on fake data to infer shapes\n",
    "        total_terms = self.theta(sample_data)[1].shape[1]\n",
    "\n",
    "        return total_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class library_basic(Library):\n",
    "    '''Implementation of library layer. Inherets from Library layer.'''\n",
    "    def __init__(self, input_dim, output_dim, diff_order, poly_order):\n",
    "        self.poly_order = poly_order\n",
    "        super().__init__(input_dim, output_dim, diff_order)\n",
    "    \n",
    "    def theta(self, input):\n",
    "        '''Calculates the library and time deriv from NN output'''\n",
    "        X, dX = input\n",
    "        samples = X.shape[0]\n",
    "\n",
    "        # Time derivatives\n",
    "        dt = dX[:, 0, :1, :]\n",
    "        time_deriv_list = torch.unbind(dt, dim=2)\n",
    "\n",
    "        # Polynomial part\n",
    "        u = torch.ones_like(X)[:, None, :]\n",
    "        for order in torch.arange(1, self.poly_order+1):\n",
    "            u = torch.cat((u, u[:, order-1:order, :] * X[:, None, :]), dim=1)\n",
    "        poly_list = torch.unbind(u, dim=2) #list with each entry corresponding to eq.\n",
    "\n",
    "        # Derivative part\n",
    "        dx = dX[:, :, 1:, :]\n",
    "        deriv_list = [torch.cat((torch.ones((samples, 1)), eq.reshape(samples, -1)), dim=1) for eq in torch.unbind(dx, dim=3)] #list with each entry corresponding to eq.\n",
    "        \n",
    "        # Combining to make  theta\n",
    "        if len(poly_list) == 1:\n",
    "            theta = torch.matmul(poly_list[0][:, :, None], deriv_list[0][:, None, :]).reshape(samples, -1) # If we have a single output, we simply calculate and flatten matrix product between polynomials and derivatives to get library\n",
    "        else:\n",
    "            theta_uv = torch.cat([torch.matmul(u[:, :, None], v[:, None, :]).reshape(samples, -1) for u, v in combinations(poly_list, 2)], 1)  # calculate all unique combinations between polynomials\n",
    "            theta_dudv = torch.cat([torch.matmul(du[:, :, None], dv[:, None, :]).reshape(samples, -1)[:, 1:] for du, dv in combinations(deriv_list, 2)], 1) # calculate all unique combinations of derivatives\n",
    "            theta_udu = torch.cat([torch.matmul(u[:, 1:, None], du[:, None, 1:]).reshape(samples, -1) for u, du in product(poly_list, deriv_list)], 1)  # calculate all unique products of polynomials and derivatives\n",
    "            theta = torch.cat([theta_uv, theta_dudv, theta_udu], dim=1)\n",
    "\n",
    "        return time_deriv_list, theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting and running deepmod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'input_dim': 2, 'hidden_dim': 20, 'layers': 5, 'output_dim': 1, 'library_function': library_basic, 'library_args':{'poly_order': 2, 'diff_order': 2}}\n",
    "\n",
    "X_input = create_deriv_data(X_train, config['library_args']['diff_order'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_network(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we'll do PI training, we need a decent estimate, otherwise we'll get shite posteriors. (There's no noise for now anyway)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "max_iterations = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Iteration | Progress | Time remaining |     Cost |      MSE |      Reg |       L1 |\n",
      "       5000    100.00%               0s   2.71e-06   2.71e-06   0.00e+00   0.00e+00 "
     ]
    }
   ],
   "source": [
    "print('| Iteration | Progress | Time remaining |     Cost |      MSE |      Reg |       L1 |')\n",
    "for iteration in torch.arange(0, max_iterations + 1):\n",
    "    prediction, time_deriv_list, theta = model(X_input)\n",
    "    loss_mse = torch.mean((prediction[0] - y_train)**2)\n",
    "    loss = loss_mse \n",
    "        \n",
    "    # Writing\n",
    "    if iteration % 100 == 0:\n",
    "        progress(iteration, 0, max_iterations, loss.item(), loss_mse.item(), 0, 0)\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that done, let's do a fit to see the result, both using least squares and bayesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.7115453e-05],\n",
       "       [ 5.2083950e-03],\n",
       "       [ 9.2110246e-02],\n",
       "       [ 1.9702425e-03],\n",
       "       [-1.1246353e+00],\n",
       "       [ 5.5794012e-02],\n",
       "       [-4.1911104e-03],\n",
       "       [ 2.2810708e-01],\n",
       "       [-8.9375928e-02]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.lstsq(theta.cpu().detach().numpy(), time_deriv_list[0].cpu().detach().numpy(), rcond=None)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which doesn't seem too bad. Now let's do some bayezzz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type II maximalization on deepmod results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = theta.detach().clone()\n",
    "y = time_deriv_list[0].detach().clone()\n",
    "\n",
    "M = X.shape[1]\n",
    "N = X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = nn.Parameter(torch.tensor(1.0))\n",
    "noise = nn.Parameter(torch.tensor(1.0))\n",
    "\n",
    "optimizer = torch.optim.Adam([cov, noise])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2381.2461]], grad_fn=<MulBackward0>) Parameter containing:\n",
      "tensor(0.1454, requires_grad=True) Parameter containing:\n",
      "tensor(0.0048, requires_grad=True)\n",
      "tensor([[-2381.2461]], grad_fn=<MulBackward0>) Parameter containing:\n",
      "tensor(0.1454, requires_grad=True) Parameter containing:\n",
      "tensor(0.0048, requires_grad=True)\n",
      "tensor([[-2381.2461]], grad_fn=<MulBackward0>) Parameter containing:\n",
      "tensor(0.1454, requires_grad=True) Parameter containing:\n",
      "tensor(0.0048, requires_grad=True)\n",
      "tensor([[-2381.2458]], grad_fn=<MulBackward0>) Parameter containing:\n",
      "tensor(0.1454, requires_grad=True) Parameter containing:\n",
      "tensor(0.0048, requires_grad=True)\n",
      "tensor([[-2381.2461]], grad_fn=<MulBackward0>) Parameter containing:\n",
      "tensor(0.1454, requires_grad=True) Parameter containing:\n",
      "tensor(0.0048, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for it in np.arange(5000):\n",
    "    alpha = 1/cov\n",
    "    beta = 1/noise**2\n",
    "    A = alpha * torch.eye(X.shape[1]) + beta * X.T @ X\n",
    "    mn = beta * torch.inverse(A) @ X.T @ y\n",
    "    E = beta/2 * (y - X @ mn).T @ (y - X @ mn) + alpha/2 * mn.T @ mn\n",
    "    \n",
    "    loss = -1 * (M/2*torch.log(alpha) + N/2 * torch.log(beta) - E - torch.sum(torch.log(torch.diag(torch.cholesky(A)))))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if it %1000 == 0:\n",
    "        print(loss, cov, noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now get a the following means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.1631e-05],\n",
       "        [ 2.9984e-03],\n",
       "        [ 9.1805e-02],\n",
       "        [ 2.1081e-03],\n",
       "        [-1.1128e+00],\n",
       "        [ 5.6161e-02],\n",
       "        [-4.7116e-03],\n",
       "        [ 2.1575e-01],\n",
       "        [-8.9609e-02]], grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0003, 0.0072, 0.0036, 0.0048, 0.0357, 0.0189, 0.0116, 0.0382, 0.0212],\n",
       "       grad_fn=<SqrtBackward>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sqrt(torch.diagonal(torch.inverse(A)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which seems pretty similar to the other results :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing deepmod with bayesian inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think we can just throw the bayesian loss in there with the mse. The massive difference in loss might be problematic though, but let's see. We also won't update the prior to be the previous posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([{'params': model.parameters()}, {'params':[cov, noise]}])\n",
    "max_iterations = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Iteration | Progress | Time remaining |     Cost |      MSE |      Reg |       L1 |\n",
      "       1000    100.00%               0s   -2.51e+03   3.53e-01   -2.51e+03   0.00e+00 "
     ]
    }
   ],
   "source": [
    "print('| Iteration | Progress | Time remaining |     Cost |      MSE |      Reg |       L1 |')\n",
    "for iteration in torch.arange(0, max_iterations + 1):\n",
    "    # NN\n",
    "    prediction, time_deriv_list, theta = model(X_input)\n",
    "    loss_mse = torch.mean((prediction[0] - y_train)**2)\n",
    "    \n",
    "    ## Bayes\n",
    "    alpha = 1/cov\n",
    "    beta = 1/noise**2\n",
    "    A = alpha * torch.eye(theta.shape[1]) + beta * theta.T @ theta\n",
    "    mn = beta * torch.inverse(A) @ theta.T @ time_deriv_list[0]\n",
    "    E = beta/2 * (time_deriv_list[0] - theta @ mn).T @ (time_deriv_list[0] - theta @ mn) + alpha/2 * mn.T @ mn\n",
    "    \n",
    "    loss_bayes = -1 * (M/2*torch.log(alpha) + N/2 * torch.log(beta) - E - torch.sum(torch.log(torch.diag(torch.cholesky(A)))))\n",
    "    \n",
    "    \n",
    "    # Full loss\n",
    "    loss = loss_mse + loss_bayes\n",
    "        \n",
    "    # Writing\n",
    "    if iteration % 100 == 0:\n",
    "        progress(iteration, 0, max_iterations, loss.item(), loss_mse.item(), loss_bayes.item(), 0)\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0065],\n",
       "        [-0.2509],\n",
       "        [-0.0021],\n",
       "        [-0.0091],\n",
       "        [-0.1137],\n",
       "        [ 0.0441],\n",
       "        [-0.0232],\n",
       "        [ 0.0882],\n",
       "        [ 0.0974]], grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that doens't work, probably because its vastly different magnitudes. How should we scale them?? Maybe use a probabilistic fuction generator as well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Bayes + MSE from the start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add some noise (maybe cholesky fails when theres no noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_samples = 500\n",
    "\n",
    "idx = np.random.permutation(y.size)\n",
    "X_train = torch.tensor(X[idx, :][:number_of_samples], dtype=torch.float32, requires_grad=True)\n",
    "y_train = torch.tensor(y[idx, :][:number_of_samples] + np.random.normal(scale=0.01, size=y[idx, :][:number_of_samples].shape), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'input_dim': 2, 'hidden_dim': 20, 'layers': 5, 'output_dim': 1, 'library_function': library_basic, 'library_args':{'poly_order': 2, 'diff_order': 2}}\n",
    "\n",
    "X_input = create_deriv_data(X_train, config['library_args']['diff_order'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = nn.Parameter(torch.tensor(1.0))\n",
    "noise = nn.Parameter(torch.tensor(1.0))\n",
    "\n",
    "model = build_network(**config)\n",
    "\n",
    "optimizer = torch.optim.Adam([{'params': model.parameters()}, {'params':[cov, noise]}])\n",
    "max_iterations = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Iteration | Progress | Time remaining |     Cost |      MSE |      Reg |       L1 |\n",
      "      10000    100.00%               0s   -3.57e+00   4.02e-02   -3.61e+00   0.00e+00 "
     ]
    }
   ],
   "source": [
    "print('| Iteration | Progress | Time remaining |     Cost |      MSE |      Reg |       L1 |')\n",
    "for iteration in torch.arange(0, max_iterations + 1):\n",
    "    # NN\n",
    "    prediction, time_deriv_list, theta = model(X_input)\n",
    "    loss_mse = torch.mean((prediction[0] - y_train)**2)\n",
    "    \n",
    "    ## Bayes\n",
    "    M = theta.shape[1]\n",
    "    N = theta.shape[0]\n",
    "\n",
    "    alpha = 1/cov**2\n",
    "    beta = 1/noise**2\n",
    "    A = alpha * torch.eye(theta.shape[1]) + beta * theta.T @ theta\n",
    "    mn = beta * torch.inverse(A) @ theta.T @ time_deriv_list[0]\n",
    "    E = beta/2 * (time_deriv_list[0] - theta @ mn).T @ (time_deriv_list[0] - theta @ mn) + alpha/2 * mn.T @ mn\n",
    "    \n",
    "    loss_bayes = -1/N * (M/2*torch.log(alpha) + N/2 * torch.log(beta) - E - torch.sum(torch.log(torch.diag(torch.cholesky(A)))))\n",
    "    \n",
    "    \n",
    "    # Full loss\n",
    "    loss = loss_mse + loss_bayes\n",
    "        \n",
    "    # Writing\n",
    "    if iteration % 100 == 0:\n",
    "        progress(iteration, 0, max_iterations, loss.item(), loss_mse.item(), loss_bayes.item(), 0)\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rescaling bayes loss with the number of samples definitely helps a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor(0.0229, requires_grad=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor(-0.0269, requires_grad=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe optimizing for the hyperparameters is not correct? Maybe optimize with set hyperparameters and then after optimize the hyperparameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing DeepMoD + Bayes with fixed hyperparams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_samples = 500\n",
    "\n",
    "idx = np.random.permutation(y.size)\n",
    "X_train = torch.tensor(X[idx, :][:number_of_samples], dtype=torch.float32, requires_grad=True)\n",
    "y_train = torch.tensor(y[idx, :][:number_of_samples], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'input_dim': 2, 'hidden_dim': 20, 'layers': 5, 'output_dim': 1, 'library_function': library_basic, 'library_args':{'poly_order': 2, 'diff_order': 2}}\n",
    "\n",
    "X_input = create_deriv_data(X_train, config['library_args']['diff_order'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = nn.Parameter(torch.tensor(0.1))\n",
    "noise = nn.Parameter(torch.tensor(0.0001))\n",
    "\n",
    "model = build_network(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "max_iterations = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Iteration | Progress | Time remaining |     Cost |      MSE |      Reg |       L1 |\n",
      "       1900     19.00%      6323589632s   1.77e-05   9.72e-06   7.99e-06   0.00e+00 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-5a02cf79ed1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \"\"\"\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('| Iteration | Progress | Time remaining |     Cost |      MSE |      Reg |       L1 |')\n",
    "for iteration in torch.arange(0, max_iterations + 1):\n",
    "    # NN\n",
    "    prediction, time_deriv_list, theta = model(X_input)\n",
    "    loss_mse = torch.mean((prediction[0] - y_train)**2)\n",
    "    \n",
    "    ## Bayes\n",
    "    M = theta.shape[1]\n",
    "    N = theta.shape[0]\n",
    "\n",
    "    alpha = 1/cov**2\n",
    "    beta = 1/noise**2\n",
    "    A = alpha * torch.eye(theta.shape[1]) + beta * theta.T @ theta\n",
    "    mn = beta * torch.inverse(A) @ theta.T @ time_deriv_list[0]\n",
    "    E = beta/2 * (time_deriv_list[0] - theta @ mn).T @ (time_deriv_list[0] - theta @ mn) + alpha/2 * mn.T @ mn\n",
    "    \n",
    "    loss_bayes = -10**-8/N * (M/2*torch.log(alpha) + N/2 * torch.log(beta) - E - torch.sum(torch.log(torch.diag(torch.cholesky(A)))))\n",
    "    \n",
    "    \n",
    "    # Full loss\n",
    "    loss = loss_mse + loss_bayes\n",
    "        \n",
    "    # Writing\n",
    "    if iteration % 100 == 0:\n",
    "        progress(iteration, 0, max_iterations, loss.item(), loss_mse.item(), loss_bayes.item(), 0)\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.2406e-04],\n",
       "        [-2.3179e-02],\n",
       "        [ 8.6250e-02],\n",
       "        [ 1.7514e-02],\n",
       "        [-1.0157e+00],\n",
       "        [-2.0395e-02],\n",
       "        [-6.8224e-02],\n",
       "        [ 2.9492e-01],\n",
       "        [-2.0452e-02]], grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.8054e-06, 1.6205e-04, 6.5226e-05, 1.3348e-04, 8.1526e-04, 3.1213e-04,\n",
       "        3.5295e-04, 1.0502e-03, 3.9953e-04], grad_fn=<SqrtBackward>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sqrt(torch.diag(torch.inverse(A)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it doesn't necessarily blow up... The problem is in the scaling. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
