{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we investigate the effect of normalization on the fitting procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from deepmod_l1.diff_library import theta_analytical\n",
    "\n",
    "#Plotting imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# Remainder imports\n",
    "from os import listdir, path, getcwd\n",
    "\n",
    "# Setting cuda\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "# Settings for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Defining output folder\n",
    "output_folder = getcwd()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 0.5\n",
    "a = 0.25\n",
    "\n",
    "x = np.linspace(-5, 5, 500, dtype=np.float32)\n",
    "t = np.linspace(0, 5, 100, dtype=np.float32)\n",
    "x_grid, t_grid = np.meshgrid(x, t, indexing='ij')\n",
    "    \n",
    "# Analytical\n",
    "time_deriv, theta = theta_analytical(x_grid, t_grid, D, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And performing lst-sq we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "xi_base = np.linalg.lstsq(theta, time_deriv, rcond=None)[0].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.4918742e-17,  2.0816682e-16,  5.0000000e-01,  4.4582393e-16,\n",
       "       -1.2127323e-16, -1.0972126e-16, -3.7296555e-17,  1.0139718e-16,\n",
       "       -7.5894152e-17], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xi_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(theta, dtype=torch.float32)\n",
    "y_train = torch.tensor(time_deriv, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(*[nn.Linear(X_train.shape[1], 1, bias=False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "iterations = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16057567298412323\n",
      "0.005893857218325138\n",
      "0.00028583381208591163\n",
      "0.00013026372471358627\n",
      "7.630345498910174e-05\n",
      "3.997812746092677e-05\n",
      "1.569796950207092e-05\n",
      "3.4312636216782266e-06\n",
      "2.764786586340051e-07\n",
      "4.198431380331158e-09\n"
     ]
    }
   ],
   "source": [
    "for it in np.arange(iterations):\n",
    "    prediction = model(X_train)\n",
    "    loss = torch.mean((prediction - y_train)**2)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if it % 1000 == 0:\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "xi = model[0].weight.detach().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.0761206e-06,  3.2607575e-10,  4.9999043e-01, -1.8266408e-05,\n",
       "        5.0569660e-10,  3.7870883e-05,  5.8061538e-05,  5.9180172e-10,\n",
       "       -1.8908288e-05], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.2274047e-10"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((xi_base - xi)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which looks okay; now on to standardizing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.mean(theta, axis=0)\n",
    "b = np.std(theta, axis=0)\n",
    "\n",
    "a[0] = 0.0 # for the ones.\n",
    "b[0] = 1.0\n",
    "\n",
    "theta_standard = (theta - a)/b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(theta_standard, dtype=torch.float32)\n",
    "y_train = torch.tensor(time_deriv, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(*[nn.Linear(X_train.shape[1], 1, bias=False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "iterations = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2830438017845154\n",
      "0.00010726555046858266\n",
      "7.093116494161222e-08\n",
      "4.345527937488214e-09\n",
      "2.2145265621276167e-09\n",
      "9.250656907155985e-10\n",
      "2.1765943214457906e-10\n",
      "1.9710988050092304e-11\n",
      "3.7908841647379954e-13\n",
      "1.5320766666977281e-15\n"
     ]
    }
   ],
   "source": [
    "for it in np.arange(iterations):\n",
    "    prediction = model(X_train)\n",
    "    loss = torch.mean((prediction - y_train)**2)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if it % 1000 == 0:\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss is definitely lower, let's see about the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.2046875e-04, -8.9323121e-10,  2.8207892e-01,  4.9862003e-10,\n",
       "        2.0298783e-09,  8.4423490e-08, -6.2131666e-10, -3.4909617e-09,\n",
       "       -6.4450063e-08], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model[0].weight.detach().numpy().squeeze()\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's transform them back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights[0] = weights[0] - np.sum(weights[1:] * a[1:])\n",
    "weights[1:] = weights[1:] / b[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "xi_standard = weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.2684195e-04, -5.6303091e-09,  4.9999994e-01,  4.3541983e-09,\n",
       "        1.8576582e-08,  1.2963892e-07, -8.6668486e-09, -3.0171726e-08,\n",
       "       -6.8665500e-08], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xi_standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.717477e-09"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((xi_base-xi_standard)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So basically same level as the other one; although the MSE is smaller :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the norm over theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.linalg.norm(theta, axis=0)\n",
    "b[0] = 1.0\n",
    "theta_norm = theta / b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(theta_norm, dtype=torch.float32)\n",
    "y_train = torch.tensor(time_deriv, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(*[nn.Linear(X_train.shape[1], 1, bias=False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "iterations = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02311607450246811\n",
      "0.00010733594535849988\n",
      "2.6500165404286236e-05\n",
      "1.1781864486692939e-05\n",
      "2.9803682082274463e-06\n",
      "3.0434139830504137e-07\n",
      "1.684245454214306e-08\n",
      "1.633518494081887e-11\n",
      "4.6763735245258786e-11\n",
      "4.7436592848226766e-11\n"
     ]
    }
   ],
   "source": [
    "for it in np.arange(iterations):\n",
    "    prediction = model(X_train)\n",
    "    loss = torch.mean((prediction - y_train)**2)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if it % 1000 == 0:\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.4915479e-06, -1.4072922e-05,  6.3074940e+01,  1.3593244e-06,\n",
       "       -1.5191477e-05, -6.4075721e-07,  1.7985615e-06, -1.5493950e-05,\n",
       "       -1.2513128e-06], dtype=float32)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model[0].weight.detach().numpy().squeeze()\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.4915479e-06, -3.9670496e-07,  5.0000000e-01,  4.0122760e-08,\n",
       "       -6.2174291e-07, -4.3958046e-09,  1.0685707e-07, -5.9886946e-07,\n",
       "       -5.9579546e-09], dtype=float32)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xi_norm = weights / b\n",
    "xi_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.      ,  35.47453 , 126.14988 ,  33.879135,  24.433697,\n",
       "       145.76562 ,  16.831469,  25.871998, 210.0239  ], dtype=float32)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.15864693, 0.5641579 , 0.11451477, 0.10927082,\n",
       "       0.65122026, 0.07168888, 0.11570308, 0.9386091 ], dtype=float32)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(theta, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4893133e-13"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((xi_base-xi_norm)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So xi_norm sucks, but that's probably because the scaling is too high? Update: training another round with high lr fixes it, but good to keep in mind it doesn't converge quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Min - max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.min(theta, axis=0)\n",
    "b = np.max(theta, axis=0) - np.min(theta, axis=0)\n",
    "\n",
    "a[0] = 0.0 # for the ones.\n",
    "b[0] = 1.0\n",
    "\n",
    "theta_minmax = (theta - a)/b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(theta_minmax, dtype=torch.float32)\n",
    "y_train = torch.tensor(time_deriv, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(*[nn.Linear(X_train.shape[1], 1, bias=False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "iterations = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13371394574642181\n",
      "0.016092004254460335\n",
      "0.0053431629203259945\n",
      "0.0018680129433050752\n",
      "0.001681806636042893\n",
      "0.00027302891248837113\n",
      "0.00011851068848045543\n",
      "5.7303361245431006e-05\n",
      "3.059543450945057e-05\n",
      "1.818890632421244e-05\n"
     ]
    }
   ],
   "source": [
    "for it in np.arange(iterations):\n",
    "    prediction = model(X_train)\n",
    "    loss = torch.mean((prediction - y_train)**2)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if it % 1000 == 0:\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-12.376632  ,  -0.07785565,  18.173344  ,   0.02831443,\n",
       "         0.44426605,   1.3437068 ,  -0.11400798,  -0.50503683,\n",
       "        -1.3617116 ], dtype=float32)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model[0].weight.detach().numpy().squeeze()\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights[0] = weights[0] - np.sum(weights[1:] * a[1:])\n",
    "weights[1:] = weights[1:] / b[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "xi_minmax = weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.1567212e+02, -1.0054930e-02,  4.9298874e-01,  1.7757695e-02,\n",
       "        5.0916437e-02,  2.9132446e-02, -4.4842806e-02, -4.4438783e-02,\n",
       "       -1.9940261e-02], dtype=float32)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xi_minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19198.146"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((xi_base-xi_minmax)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is baddd...... No idea why though... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.0760186e-13,  5.0000006e-01,  3.5277884e-07, -2.4898915e-12,\n",
       "       -8.8995215e-08, -5.4412334e-07,  1.7290317e-12,  3.0291073e-08],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.lstsq(theta_minmax, time_deriv, rcond=None)[0].squeeze()[1:] / b[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "456.73785"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.lstsq(theta_minmax, time_deriv, rcond=None)[0].squeeze()[0] - np.sum(np.linalg.lstsq(theta_minmax, time_deriv, rcond=None)[0].squeeze()[1:] * a[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it's def not related to our implementation though.... Seems to be numerical errors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fitting there doesn't seem to be much difference; we seems to get same results for standardized and nothing. In future might be important to compare coeffs. Norming doesn't seem great way because the number if too big\n",
    "\n",
    "**Conclusion**: Standardize theta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
