{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from phimal_utilities.data import Dataset\n",
    "from phimal_utilities.data.burgers import BurgersDelta\n",
    "from phimal_utilities.analysis import load_tensorboard\n",
    "\n",
    "from DeePyMoD_SBL.deepymod_torch.library_functions import library_1D_in\n",
    "from DeePyMoD_SBL.deepymod_torch.DeepMod import DeepModDynamic\n",
    "from DeePyMoD_SBL.deepymod_torch.training import train_dynamic\n",
    "from DeePyMoD_SBL.deepymod_torch.estimators import Threshold, Clustering, ClusteringL1, PDEFIND\n",
    "from pysindy.optimizers import STLSQ\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = 0.1\n",
    "A = 1.0\n",
    "\n",
    "# Making grid\n",
    "x = np.linspace(-3, 4, 100)\n",
    "t = np.linspace(0.5, 5.0, 50)\n",
    "x_grid, t_grid = np.meshgrid(x, t, indexing='ij')\n",
    "\n",
    "dataset = Dataset(BurgersDelta, v=v, A=A)\n",
    "X_train, y_train, rand_idx = dataset.create_dataset(x_grid.reshape(-1, 1), t_grid.reshape(-1, 1), n_samples=1000, noise=0.2, random=True, return_idx=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = dataset.library(x_grid.reshape(-1, 1), t_grid.reshape(-1, 1))\n",
    "dt = dataset.time_deriv(x_grid.reshape(-1, 1), t_grid.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_normed = theta / np.linalg.norm(theta, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building train STLSQ method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysindy.optimizers import STLSQ\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainSTRidge(R, Ut, lam, d_tol, maxit = 25, STR_iters = 10, l0_penalty = None, normalize = 2, split = 0.8, print_best_tol = False):\n",
    "        \"\"\"\n",
    "        This function trains a predictor using STRidge.\n",
    "\n",
    "        It runs over different values of tolerance and trains predictors on a training set, then evaluates them \n",
    "        using a loss function on a holdout set.\n",
    "\n",
    "        Please note published article has typo.  Loss function used here for model selection evaluates fidelity using 2-norm,\n",
    "        not squared 2-norm.\n",
    "        \"\"\"\n",
    "\n",
    "        # Split data into 80% training and 20% test, then search for the best tolderance.\n",
    "        np.random.seed(0) # for consistancy\n",
    "        n,_ = R.shape\n",
    "        train = np.random.choice(n, int(n*split), replace = False)\n",
    "        test = [i for i in np.arange(n) if i not in train]\n",
    "        TrainR = R[train,:]\n",
    "        TestR = R[test,:]\n",
    "        TrainY = Ut[train,:]\n",
    "        TestY = Ut[test,:]\n",
    "        D = TrainR.shape[1]       \n",
    "\n",
    "        # Set up the initial tolerance and l0 penalty\n",
    "        d_tol = float(d_tol)\n",
    "        tol = d_tol\n",
    "        if l0_penalty == None: l0_penalty = 0.001*np.linalg.cond(R)\n",
    "\n",
    "        # Get the standard least squares estimator\n",
    "        w = np.zeros((D,1))\n",
    "        w_best = np.linalg.lstsq(TrainR, TrainY)[0]\n",
    "        err_best = np.linalg.norm(TestY - TestR.dot(w_best), 2) + l0_penalty*np.count_nonzero(w_best)\n",
    "        tol_best = 0\n",
    "\n",
    "        # Now increase tolerance until test performance decreases\n",
    "        for iter in range(maxit):\n",
    "\n",
    "            # Get a set of coefficients and error\n",
    "            opt = STLSQ(threshold=tol, alpha=lam, fit_intercept=False)\n",
    "            w = opt.fit(R, Ut).coef_.T\n",
    "            err = np.linalg.norm(TestY - TestR.dot(w), 2) + l0_penalty*np.count_nonzero(w)\n",
    "\n",
    "            # Has the accuracy improved?\n",
    "            if err <= err_best:\n",
    "                err_best = err\n",
    "                w_best = w\n",
    "                tol_best = tol\n",
    "                tol = tol + d_tol\n",
    "\n",
    "            else:\n",
    "                tol = max([0,tol - 2*d_tol])\n",
    "                d_tol  = 2*d_tol / (maxit - iter)\n",
    "                tol = tol + d_tol\n",
    "        return w_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainSTLSQ(X, y, alpha=1e-5, delta_threshold=1.0, max_iterations=25, test_size=0.2, random_state=0):\n",
    "        '''Train STLSQ. Assumes data already normalized'''\n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "        \n",
    "        # Set up the initial tolerance l0 penalty and estimates\n",
    "        l0 = 1e-3 * np.linalg.cond(X)\n",
    "        delta_t = delta_threshold # for interal use, can be updated\n",
    "        optimizer = STLSQ(threshold=0, alpha=0.0, fit_intercept=False) # Now similar to LSTSQ\n",
    "        \n",
    "        # Initial estimate\n",
    "        y_predict = optimizer.fit(X_train, y_train).predict(X_test)\n",
    "        min_loss = np.linalg.norm(y_predict - y_test, 2) + l0 * np.count_nonzero(optimizer.coef_)\n",
    "        \n",
    "        # Now set parameters of optimizer and start thresholding\n",
    "        optimizer = optimizer.set_params(threshold=delta_threshold, alpha=alpha)\n",
    "        for iteration in np.arange(max_iterations):\n",
    "            y_predict = optimizer.fit(X_train, y_train).predict(X_test)\n",
    "            loss = np.linalg.norm(y_predict - y_test, 2) + l0 * np.count_nonzero(optimizer.coef_)\n",
    "            \n",
    "            if loss <= min_loss:\n",
    "                min_loss = loss\n",
    "                optimizer.set_params(threshold=optimizer.threshold + delta_t)\n",
    "            else: # if loss increases, we need to a) lower the current threshold and/or decrease step size\n",
    "                new_lower_tol = np.max([0, optimizer.threshold - 2 * delta_t])\n",
    "                delta_t = 2 * delta_t / (max_iterations - iteration)\n",
    "                optimizer.set_params(threshold=new_lower_tol + delta_t)\n",
    "\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = 0.5 * np.std(dt) * np.random.normal(size=dt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [-9.48923445],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainSTLSQ(theta_normed, dt + noise).coef_.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:30: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0.        ],\n",
       "       [  0.        ],\n",
       "       [  7.36456104],\n",
       "       [  0.        ],\n",
       "       [-10.59050087],\n",
       "       [  0.        ],\n",
       "       [  0.        ],\n",
       "       [  0.        ],\n",
       "       [  0.        ]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainSTRidge(theta_normed, dt + noise, lam=1e-5, d_tol=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
