{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from phimal_utilities.data import Dataset\n",
    "from phimal_utilities.data.burgers import BurgersDelta\n",
    "from phimal_utilities.analysis import load_tensorboard\n",
    "\n",
    "from DeePyMoD_SBL.deepymod_torch.library_functions import library_1D_in\n",
    "from DeePyMoD_SBL.deepymod_torch.DeepMod import DeepModDynamic\n",
    "from DeePyMoD_SBL.deepymod_torch.training import train_dynamic\n",
    "from DeePyMoD_SBL.deepymod_torch.estimators import Threshold, Clustering, PDEFIND\n",
    "from pysindy.optimizers import STLSQ\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = 0.1\n",
    "A = 1.0\n",
    "\n",
    "# Making grid\n",
    "x = np.linspace(-3, 4, 100)\n",
    "t = np.linspace(0.5, 5.0, 50)\n",
    "x_grid, t_grid = np.meshgrid(x, t, indexing='ij')\n",
    "\n",
    "dataset = Dataset(BurgersDelta, v=v, A=A)\n",
    "#X_train, y_train, rand_idx = dataset.create_dataset(x_grid.reshape(-1, 1), t_grid.reshape(-1, 1), n_samples=1000, noise=0.2, random=True, return_idx=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = dataset.library(x_grid.reshape(-1, 1), t_grid.reshape(-1, 1))\n",
    "dt = dataset.time_deriv(x_grid.reshape(-1, 1), t_grid.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_normed = theta / np.linalg.norm(theta, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building train STLSQ method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysindy.optimizers import STLSQ\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainSTRidge(R, Ut, lam=1e-5, d_tol=1.0, maxit = 50, STR_iters = 10, l0_penalty = None, normalize = 2, split = 0.8, print_best_tol = False):\n",
    "        \"\"\"\n",
    "        This function trains a predictor using STRidge.\n",
    "\n",
    "        It runs over different values of tolerance and trains predictors on a training set, then evaluates them \n",
    "        using a loss function on a holdout set.\n",
    "\n",
    "        Please note published article has typo.  Loss function used here for model selection evaluates fidelity using 2-norm,\n",
    "        not squared 2-norm.\n",
    "        \"\"\"\n",
    "\n",
    "        # Split data into 80% training and 20% test, then search for the best tolderance.\n",
    "        np.random.seed(0) # for consistancy\n",
    "        n,_ = R.shape\n",
    "        train = np.random.choice(n, int(n*split), replace = False)\n",
    "        test = [i for i in np.arange(n) if i not in train]\n",
    "        TrainR = R[train,:]\n",
    "        TestR = R[test,:]\n",
    "        TrainY = Ut[train,:]\n",
    "        TestY = Ut[test,:]\n",
    "        D = TrainR.shape[1]       \n",
    "\n",
    "        # Set up the initial tolerance and l0 penalty\n",
    "        d_tol = float(d_tol)\n",
    "        tol = d_tol\n",
    "        if l0_penalty == None: l0_penalty = 0.001*np.linalg.cond(R)\n",
    "\n",
    "        # Get the standard least squares estimator\n",
    "        w = np.zeros((D,1))\n",
    "        w_best = np.linalg.lstsq(TrainR, TrainY)[0]\n",
    "        err_best = np.linalg.norm(TestY - TestR.dot(w_best), 2) + l0_penalty*np.count_nonzero(w_best)\n",
    "        tol_best = 0\n",
    "\n",
    "        # Now increase tolerance until test performance decreases\n",
    "        for iter in range(maxit):\n",
    "\n",
    "            # Get a set of coefficients and error\n",
    "            opt = STLSQ(threshold=tol, alpha=lam, fit_intercept=False)\n",
    "            w = opt.fit(TrainR, TrainY).coef_.T\n",
    "            err = np.linalg.norm(TestY - TestR.dot(w), 2) + l0_penalty*np.count_nonzero(w)\n",
    "        \n",
    "            # Has the accuracy improved?\n",
    "            if err <= err_best:\n",
    "                err_best = err\n",
    "                w_best = w\n",
    "                tol_best = tol\n",
    "                tol = tol + d_tol\n",
    "\n",
    "            else:\n",
    "                tol = max([0,tol - 2*d_tol])\n",
    "                d_tol  = 2*d_tol / (maxit - iter)\n",
    "                tol = tol + d_tol\n",
    "        return w_best, tol_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainSTLSQ(X, y, lam=1e-5, d_tol=1.0, maxit = 50, STR_iters = 10, l0_penalty = None, normalize = 2, split = 0.8, print_best_tol = False):\n",
    "        \"\"\"\n",
    "        This function trains a predictor using STRidge.\n",
    "\n",
    "        It runs over different values of tolerance and trains predictors on a training set, then evaluates them \n",
    "        using a loss function on a holdout set.\n",
    "\n",
    "        Please note published article has typo.  Loss function used here for model selection evaluates fidelity using 2-norm,\n",
    "        not squared 2-norm.\n",
    "        \"\"\"\n",
    "\n",
    "        # Split data into 80% training and 20% test, then search for the best tolderance.\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=split, random_state=0)\n",
    "\n",
    "        # Set up the initial tolerance and l0 penalty\n",
    "        tol = d_tol\n",
    "        if l0_penalty == None: l0_penalty = 0.001*np.linalg.cond(X)\n",
    "\n",
    "        # Get the standard least squares estimator\n",
    "        w_best = np.linalg.lstsq(X_train, y_train)[0]\n",
    "        err_best = np.linalg.norm(y_test - X_test.dot(w_best), 2) + l0_penalty*np.count_nonzero(w_best)\n",
    "        \n",
    "        opt = STLSQ(threshold=tol, alpha=lam, fit_intercept=False)\n",
    "        # Now increase tolerance until test performance decreases\n",
    "        for iter in range(maxit):\n",
    "            # Get a set of coefficients and error\n",
    "            opt.set_params(threshold=tol)\n",
    "            w = opt.fit(X_train, y_train).coef_.T\n",
    "            err = np.linalg.norm(y_test - X_test.dot(w), 2) + l0_penalty*np.count_nonzero(w)\n",
    "        \n",
    "            # Has the accuracy improved?\n",
    "            if err <= err_best:\n",
    "                err_best = err\n",
    "                w_best = w\n",
    "                tol_best = tol\n",
    "                tol = tol + d_tol\n",
    "\n",
    "            else:\n",
    "                tol = max([0,tol - 2*d_tol])\n",
    "                d_tol  = 2*d_tol / (maxit - iter)\n",
    "                tol = tol + d_tol\n",
    "        return w, w_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = 1.0 * np.std(dt) * np.random.normal(size=dt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.        ,   0.        ,   7.3884286 ,   0.        ,\n",
       "        -10.45544219,   0.        ,   0.        ,   0.        ,\n",
       "          0.        ]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainSTLSQ(theta_normed, dt + noise).coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:30: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[  0.        ],\n",
       "        [  0.        ],\n",
       "        [  7.41875247],\n",
       "        [  0.        ],\n",
       "        [-10.56869588],\n",
       "        [  0.        ],\n",
       "        [  0.        ],\n",
       "        [  0.        ],\n",
       "        [  0.        ]]), 7.418604651162793)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainSTRidge(theta_normed, dt + noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:30: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0.        ],\n",
       "       [  0.        ],\n",
       "       [  7.03695291],\n",
       "       [  0.        ],\n",
       "       [-10.63650339],\n",
       "       [  0.        ],\n",
       "       [  0.        ],\n",
       "       [  0.        ],\n",
       "       [  0.        ]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainSTRidge_bad(theta_normed, dt + noise, lam=1e-5, d_tol=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 5.19830877e-15],\n",
       "       [ 2.39825521e-16],\n",
       "       [ 7.33313063e+00],\n",
       "       [ 4.61002764e-16],\n",
       "       [-1.05619424e+01],\n",
       "       [ 4.98104162e-15],\n",
       "       [-3.99723657e-15],\n",
       "       [ 1.61331113e-15],\n",
       "       [-4.15021750e-15]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.lstsq(theta_normed, dt)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.69785190e-14, -2.33973202e-14,  7.33313063e+00,\n",
       "        -7.74398621e-14, -1.05619424e+01,  4.12910755e-13,\n",
       "         1.17878114e-13, -6.93212616e-14, -2.67738544e-13]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = STLSQ(threshold=0.0, alpha=0.0, fit_intercept=False) # Now similar to LSTSQ\n",
    "optimizer.fit(theta_normed, dt).coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = theta_normed\n",
    "Ut = dt\n",
    "\n",
    "split=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0) # for consistancy\n",
    "n,_ = R.shape\n",
    "train = np.random.choice(n, int(n*split), replace = False)\n",
    "test = [i for i in np.arange(n) if i not in train]\n",
    "TrainR = R[train,:]\n",
    "TestR = R[test,:]\n",
    "TrainY = Ut[train,:]\n",
    "TestY = Ut[test,:]\n",
    "D = TrainR.shape[1]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(R, Ut, train_size=0.8, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 9)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(TrainY[:1000]  == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00182186],\n",
       "       [0.12783041],\n",
       "       [0.00254644],\n",
       "       [0.00061087],\n",
       "       [0.00294557]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 1)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainSTLSQ(X, y, alpha=1e-5, delta_threshold=1.0, max_iterations=100, test_size=0.2, random_state=0):\n",
    "        '''Train STLSQ. Assumes data already normalized'''\n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "        \n",
    "        # Set up the initial tolerance l0 penalty and estimates\n",
    "        l0 = 1e-3 * np.linalg.cond(X)\n",
    "        delta_t = delta_threshold # for interal use, can be updated\n",
    "      \n",
    "        # Initial estimate\n",
    "        optimizer = STLSQ(threshold=0, alpha=0.0, fit_intercept=False) # Now similar to LSTSQ\n",
    "        y_predict = optimizer.fit(X_train, y_train).predict(X_test)\n",
    "        min_loss = np.linalg.norm(y_predict - y_test, 2) + l0 * np.count_nonzero(optimizer.coef_)\n",
    "        \n",
    "        # Setting alpha and tolerance\n",
    "        best_threshold = delta_t\n",
    "        threshold = delta_t\n",
    "\n",
    "        for iteration in np.arange(max_iterations):\n",
    "            optimizer.set_params(alpha=alpha, threshold=threshold)\n",
    "            y_predict = optimizer.fit(X_train, y_train).predict(X_test)\n",
    "            loss = np.linalg.norm(y_predict - y_test, 2) + l0 * np.count_nonzero(optimizer.coef_)\n",
    "    \n",
    "            if (loss <= min_loss) and not (np.all(optimizer.coef_ == 0)):\n",
    "                min_loss = loss\n",
    "                best_threshold = threshold\n",
    "                threshold += delta_threshold\n",
    "               \n",
    "            else: # if loss increases, we need to a) lower the current threshold and/or decrease step size\n",
    "                new_lower_threshold = np.max([0, threshold - 2 * delta_t])\n",
    "                delta_t = 2 * delta_t / (max_iterations - iteration)\n",
    "                threshold = new_lower_threshold + delta_t\n",
    "        \n",
    "        optimizer.set_params(alpha=alpha, threshold=best_threshold)\n",
    "        optimizer.fit(X_train, y_train)\n",
    "        \n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
